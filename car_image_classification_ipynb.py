# -*- coding: utf-8 -*-
"""_Hyug&Won Car Image Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6Uza4HQGGpFYpyFteya4satyirG9n8Q

# Pre_processing
"""

!nvidia-smi -L

from google.colab import drive
drive.mount('/content/gdrive')

unzip /content/gdrive/MyDrive/samples1/resized.zip -d /content

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x # 코랩에서 텐써플로우 1.x 버전으로 코드를 수행하기 위한 명령어

# Commented out IPython magic to ensure Python compatibility.
## Import module 
# %cd /content/gdrive/MyDrive

import loader_hyug
import os 
import re
import cv2
import csv
import numpy as np
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

"""## Resize"""

## Resize & create file by order:asc
# # for modeling

# path_raw = '/content/gdrive/MyDrive/Hyug_Won/train'
# path_re = "/content/gdrive/MyDrive/Hyug_Won/raw/car_resize"

# loader_hyug.resize(path_raw, path_re, height=256, width=144)  #default = 128*128

"""## Load X : Features"""

## Load X : Features

path_re = '/content/train_data2'
 

def image_load(path):
    file_list = os.listdir(path)
    
    file_name = []
    for i in file_list:
        a = int(re.sub('[^0-9]', '', i)) 
        
        file_name.append(a)
    file_name.sort()  
    file_res = []
    
    for j in file_name:
        file_res.append(path+'/sample'+ str(j) + '.jpg' )
    image = []
    for k in file_res:
        img = cv2.imread(k)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        image.append(img)
    return np.array(image)

resize = image_load(path_re)
resize.shape

## Check out first img(resized)

img = resize[0, : ]
plt.figure()
plt.imshow(img)
print(resize.shape)

## Make X (feature)

X = resize
X.shape

"""## Create Label

### Create Csv file
"""

## Make csv file in path_re

#path_re = "D:\\LSS_project\\Data\\re.csv"
#loader_hyug.csv_maker_84(path_re, k1=10, k2=600)

"""### Load y : label"""

path = "/content/gdrive/MyDrive/Final_Project/final_lable.csv"
y = loader_hyug.label_load(path,label_cnt=3)  
y.shape

"""## Scaling"""

X = X.astype('float')
X = X/255
X.shape

"""## Check X, y"""

## Confirm X, y
print(X.shape)  
print(y.shape, end='\n\n\n')  

# print("#####Check out : X#####")
# print(X, end='\n\n\n')
# print("#####Check out : y#####")
# print(y)

"""# Modeling"""

## module import
import tensorflow as tf # tensorflow 2.0
from tensorflow.keras.models import Sequential, save_model,load_model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Activation
from tensorflow.keras.layers import Dropout, BatchNormalization, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np


## 전이학습
from tensorflow.keras.applications import ResNet50 as ResNet50

from tensorflow.keras.applications.vgg16 import VGG16                        
from tensorflow.keras.applications.vgg19 import VGG19

## Check tensorFlow version

print (tf.__version__)

# 훈련/테스트 데이터를 0.7/0.3의 비율로 분리합니다.
x_train, x_val, y_train, y_val = train_test_split(X, y, 
                                                test_size = 0.3, 
                                                random_state = 343)

# Checkout
print(x_train.shape)
print(x_val.shape)  
print(y_train.shape)
print(y_val.shape)

## ResNet options

#ResNet = ResNet50(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))
#ResNet.summary()

# vgg19 = VGG19(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))
# vgg19.summary()

vgg16 = VGG16(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))
vgg16.summary()

for i, layers in enumerate(ResNet.layers[:-156]):
  print(i,layers.name)

## 가중치 초기값 : imagenet
# layer.trainable=True : 동결 해제 (default)
# layer.trainable=False : 동결 (option)

# for layer in ResNet.layers[:-156]:
#     layer.trainable = False

for layer in vgg16.layers[:-4]:
     layer.trainable = False

ResNet.summary()

#vgg16.summary()

# 신경망 객체 생성
model = Sequential()

# stacking vgg16
model.add(vgg16)

# Reshape : Flatten 
model.add(Flatten())

# 완전연결계층1
model.add(Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.005)))  #default node num = 4096
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))  #traditional range : 0.2~0.5

# 완전연결계층2
model.add(Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.005)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))

# 출력층(softmax)
model.add(Dense(3))  # class : 3
model.add(Activation('softmax'))

# Check out model 
model.summary()

model.compile(loss='categorical_crossentropy', 
              optimizer=Adam(lr = 0.00005), 
              metrics=['accuracy'])

# model.compile(loss='binary_crossentropy', 
#               optimizer="adam", 
#               metrics=['accuracy'])

epochs = 25
batch_size=50
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)

hist = model.fit(x_train, y_train, 
                 validation_data=(x_val, y_val), 
                 epochs=epochs, 
                 batch_size=batch_size,
                 callbacks = [early_stopping])

# list all data in history
print(hist.history.keys())

## Visualization : Accuracy
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])

plt.ylabel('accuracy')
plt.xlabel('epochs')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

## Visualization : Loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])

plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

## Check out accuracy

scores = model.evaluate(x_train, y_train, verbose=1)
scores2 = model.evaluate(x_val, y_val, verbose=1)

print("Vgg16 train Error : %.2f%%" % (100-scores[1]*100))
print("Vgg16 val Error : %.2f%%" % (100-scores2[1]*100))

## Save model : .h5(Hdf5 type file)

#save_path = "/content/gdrive/MyDrive/Final_Project/weight2.h5"
save_model(model, save_path)

#### Model Test ####

weight=load_model("/content/gdrive/MyDrive/Final_Project/weight3.h5")

raw_path = '/content/gdrive/MyDrive/test/test2'
resize_path = '/content/gdrive/MyDrive/test/resize2'

def resize(raw_path=None, resize_path=None, height=128, width=128):
    file_list = os.listdir(raw_path)
    file_name = []
    for i in file_list:
        a = int(re.sub('[^0-9]', '', i))  
        file_name.append(a)

    for i, k in enumerate(file_list):
        img = cv2.imread(raw_path + '/' + k)
        resize_img1 = cv2.resize(img, (height, width), interpolation=cv2.INTER_CUBIC)
        x = str(file_name[i])
        cv2.imwrite(resize_path + "/" + x + ".jpg", resize_img1)     

    plt.imshow(resize_img1)
    plt.show()  
    print("img shape", resize_img1.shape)  

resize(raw_path, resize_path, 256, 256)

def image_load2(path):
    file_list = os.listdir(path)
    
    file_name = []
    for i in file_list:
        a = int(re.sub('[^0-9]', '', i)) 
        
        file_name.append(a)
    file_name.sort()  
    
    file_res = []
    print(file_name)
    for j in file_name:
        file_res.append('%s/%d.jpg' %(path,j) )
    
    print(file_res)
    image = []
    for k in file_res:
        img = cv2.imread(k)
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        print(img.shape)
        image.append(img)
    
    return np.array(image, dtype=object)
test_image = image_load2(resize_path)
test_image.shape

x_train = test_image.astype('float32')
x_train /= 255

y = []
for i in range(x_train.shape[0]):
    y.append(np.expand_dims(x_train[i],axis = 0) )

for idx, j in enumerate(y):
    result=weight.predict(j)
    #print(result)
    a = np.argmax(result)
    if a == 0: 
        print(idx + 1, "Avante : {}%".format(result[0][a] * 100))
    elif a==1:
        print(idx + 1, "Grandeur : {}%".format(result[0][a] * 100))
    else:
        print(idx + 1, "Sonata : {}%".format(result[0][a] * 100))
